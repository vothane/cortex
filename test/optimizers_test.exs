defmodule OptimizerTest do
  use ExUnit.Case
  doctest Optimizer
  
  test "sgd" do
    {status, sgd} = SGD.sgd(%{w_: nil, momentum: 0.1, learning_rate: 0.1})
    updated = SGD.update!(sgd, Nx.tensor([[0.5, 0.5]]), Nx.tensor([[0.25, 0.25]]))
    assert updated == Nx.tensor([[0.4775, 0.4775]])
  end

  test "RMSprop" do
    {status, rmsp} = RMSprop.rmsp(%{})
    weights = Nx.tensor([[ 0.07085474, -0.28213255, -0.05047395],
                         [ 0.11325308,  0.26474701, -0.3159735 ],
                         [ 0.30152896, -0.07805746,  0.29964702],
                         [ 0.06622828,  0.20798035,  0.0472517 ],
                         [ 0.0810025,  -0.13561347,  0.05491823],
                         [ 0.15812765,  0.22661757,  0.16132814],
                         [ 0.1252624,   0.23051703, -0.11214638],
                         [ 0.10801632, -0.03107005, -0.07456477],
                         [-0.05640786, -0.06230978, -0.11549653],
                         [ 0.07710858, -0.0441155,   0.29965875]])
    
    gradients = Nx.tensor([[ 0.0,        0.0,        0.0        ],
                           [ 0.0,        0.0,        0.0        ],
                           [-0.02163943, 0.01110656, 0.010532860],
                           [-0.00781209, 0.0040096,  0.003802490],
                           [ 0.0,        0.0,        0.0        ],
                           [ 0.0 ,       0.0,        0.0        ],
                           [-0.11544177, 0.05925117, 0.056190600],
                           [-0.05431437, 0.02787717, 0.026437200],
                           [ 0.0,        0.0,        0.0        ],
                           [ 0.0,        0.0,        0.0        ]])
    
    IO.inspect(RMSprop.update!(rmsp, weights, gradients))
  end

  test "Adam" do
    {status, adam} = Adam.adam(%{})
    weights = Nx.tensor([[ 0.07085474, -0.28213255, -0.05047395],
                         [ 0.11325308,  0.26474701, -0.3159735 ],
                         [ 0.30152896, -0.07805746,  0.29964702],
                         [ 0.06622828,  0.20798035,  0.0472517 ],
                         [ 0.0810025,  -0.13561347,  0.05491823],
                         [ 0.15812765,  0.22661757,  0.16132814],
                         [ 0.1252624,   0.23051703, -0.11214638],
                         [ 0.10801632, -0.03107005, -0.07456477],
                         [-0.05640786, -0.06230978, -0.11549653],
                         [ 0.07710858, -0.0441155,   0.29965875]])
    
    gradients = Nx.tensor([[ 0.0,        0.0,        0.0        ], 
                           [ 0.0,        0.0,        0.0        ], 
                           [-0.02163943, 0.01110656, 0.010532860], 
                           [-0.00781209, 0.0040096,  0.003802490], 
                           [ 0.0,        0.0,        0.0        ], 
                           [ 0.0 ,       0.0,        0.0        ], 
                           [-0.11544177, 0.05925117, 0.056190600], 
                           [-0.05431437, 0.02787717, 0.026437200], 
                           [ 0.0,        0.0,        0.0        ], 
                           [ 0.0,        0.0,        0.0        ]])
    
    IO.inspect(Adam.update!(adam, weights, gradients))
  end
end  